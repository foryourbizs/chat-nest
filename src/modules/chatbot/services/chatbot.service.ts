import { BadRequestException, Injectable, Logger } from '@nestjs/common';
import { ConfigService } from '@nestjs/config';
import OpenAI from 'openai';
import {
  DEFAULT_OPENAI_CONFIG,
  OpenAIConfig,
  getSelectedModel,
} from 'src/config/openai.config';
import { CharacterService } from '../../character/character.service';
import { ConversationService } from '../../conversation/conversation.service';
import { Message, MessageRole } from '../../message/message.entity';
import { MessageService } from '../../message/message.service';
import { SendMessageDto } from '../dto/send-message.dto';

interface ChatResponse {
  content: string;
  tokenCount: number;
  model: string;
  finishReason?: string;
  usage?: {
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
  };
}

@Injectable()
export class ChatbotService {
  private readonly logger = new Logger(ChatbotService.name);
  private readonly openai: OpenAI;
  private readonly selectedModel: string;

  constructor(
    private readonly messageService: MessageService,
    private readonly conversationService: ConversationService,
    private readonly characterService: CharacterService,
    private readonly configService: ConfigService,
  ) {
    // OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    this.openai = OpenAIConfig.getInstance(configService);
    // í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” ê¸°ë³¸ê°’ìœ¼ë¡œ ëª¨ë¸ ì„ íƒ
    this.selectedModel = getSelectedModel(configService);
    this.logger.log(
      `ğŸ¤– OpenAI í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ì‚¬ìš© ëª¨ë¸: ${this.selectedModel}`,
    );
  }

  /**
   * ì‚¬ìš©ì ë©”ì‹œì§€ ì „ì†¡ ë° AI ì‘ë‹µ ìƒì„±
   */
  async sendMessage(
    userId: number,
    sendMessageDto: SendMessageDto,
  ): Promise<{
    userMessage: Message;
    assistantMessage: Message;
  }> {
    const { content, conversationId } = sendMessageDto;

    try {
      // ëŒ€í™” ì¡´ì¬ í™•ì¸ (í…ŒìŠ¤íŠ¸ìš© - ì†Œìœ ê¶Œ í™•ì¸ ê°„ì†Œí™”)
      const conversation =
        await this.conversationService.conversationRepository.findOne({
          where: { id: conversationId, isActive: true },
        });

      if (!conversation) {
        throw new BadRequestException('ëŒ€í™”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.');
      }

      // ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
      const userMessage = await this.messageService.createMessage({
        content,
        role: MessageRole.USER,
        conversationId,
        tokenCount: this.estimateTokenCount(content),
      });

      // ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
      const { systemPrompt, messages } =
        await this.messageService.buildConversationContext(
          conversationId,
          userId,
        );

      // AI ì‘ë‹µ ìƒì„±
      const aiResponse = await this.generateOpenAIResponse(
        systemPrompt,
        messages,
        content,
      );

      // AI ì‘ë‹µ ë©”ì‹œì§€ ì €ì¥
      const assistantMessage = await this.messageService.createMessage({
        content: aiResponse.content,
        role: MessageRole.ASSISTANT,
        conversationId,
        tokenCount: aiResponse.tokenCount,
        metadata: {
          model: aiResponse.model,
          timestamp: new Date().toISOString(),
        },
      });

      // ìºë¦­í„° ì‚¬ìš© íšŸìˆ˜ ì¦ê°€
      const conversationWithCharacter =
        await this.conversationService.findConversationWithCharacter(
          conversationId,
        );
      if (conversationWithCharacter?.character) {
        await this.characterService.incrementUsageCount(
          conversationWithCharacter.character.id,
        );
      }

      this.logger.log(
        `AI response generated for conversation ${conversationId}, user ${userId}`,
      );

      return {
        userMessage,
        assistantMessage,
      };
    } catch (error) {
      this.logger.error(
        `Failed to send message for conversation ${conversationId}`,
        error,
      );
      throw error;
    }
  }

  /**
   * OpenAI APIë¥¼ ì‚¬ìš©í•œ ì‹¤ì œ AI ì‘ë‹µ ìƒì„±
   */
  private async generateOpenAIResponse(
    systemPrompt: string,
    messages: Message[],
    newUserMessage: string,
  ): Promise<ChatResponse> {
    try {
      // ì»¨í…ìŠ¤íŠ¸ ìµœì í™”: GPT-4 Turboì˜ 128K ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°ë¥¼ ìµœëŒ€í•œ í™œìš©
      const maxContextMessages = DEFAULT_OPENAI_CONFIG.maxContextMessages;
      const contextMessages = await this.optimizeContextMessages(
        messages,
        maxContextMessages,
      );

      // ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ë¥¼ OpenAI Chat Completion í˜•ì‹ìœ¼ë¡œ ë³€í™˜
      const chatMessages: OpenAI.Chat.Completions.ChatCompletionMessageParam[] =
        [
          {
            role: 'system',
            content: systemPrompt,
          },
          // ìµœì í™”ëœ ì»¨í…ìŠ¤íŠ¸ ë©”ì‹œì§€ë“¤ ì¶”ê°€
          ...contextMessages.map((msg) => ({
            role: msg.role as 'user' | 'assistant',
            content: msg.content,
          })),
          // ìƒˆë¡œìš´ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
          {
            role: 'user',
            content: newUserMessage,
          },
        ];

      this.logger.debug(
        `OpenAI API í˜¸ì¶œ ì‹œì‘ - ëª¨ë¸: ${this.selectedModel}, ë©”ì‹œì§€: ${chatMessages.length}ê°œ`,
      );

      // OpenAI API í˜¸ì¶œ (ë™ì  ëª¨ë¸ ì„ íƒ)
      const response = await this.openai.chat.completions.create({
        model: this.selectedModel,
        messages: chatMessages,
        max_tokens: DEFAULT_OPENAI_CONFIG.maxTokens,
        temperature: DEFAULT_OPENAI_CONFIG.temperature,
        top_p: DEFAULT_OPENAI_CONFIG.topP,
        frequency_penalty: DEFAULT_OPENAI_CONFIG.frequencyPenalty,
        presence_penalty: DEFAULT_OPENAI_CONFIG.presencePenalty,
      });

      const choice = response.choices[0];
      const content = choice.message?.content || '';

      if (!content) {
        throw new Error('OpenAI ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤');
      }

      this.logger.debug(
        `OpenAI API ì‘ë‹µ ì„±ê³µ - í† í°: ${response.usage?.total_tokens || 0}`,
      );

      return {
        content,
        tokenCount:
          response.usage?.total_tokens || this.estimateTokenCount(content),
        model: response.model,
        finishReason: choice.finish_reason || undefined,
        usage: response.usage
          ? {
              promptTokens: response.usage.prompt_tokens,
              completionTokens: response.usage.completion_tokens,
              totalTokens: response.usage.total_tokens,
            }
          : undefined,
      };
    } catch (error) {
      this.logger.error('OpenAI API í˜¸ì¶œ ì‹¤íŒ¨', error);

      // OpenAI API ì—ëŸ¬ ìƒì„¸ ì²˜ë¦¬
      if (error?.error?.code === 'insufficient_quota') {
        throw new BadRequestException(
          'OpenAI API ì‚¬ìš©ëŸ‰ì„ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•´ì£¼ì„¸ìš”.',
        );
      } else if (error?.error?.code === 'invalid_api_key') {
        throw new BadRequestException('OpenAI API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.');
      } else if (error?.error?.code === 'rate_limit_exceeded') {
        throw new BadRequestException(
          'ë„ˆë¬´ ë§ì€ ìš”ì²­ì´ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.',
        );
      }

      throw new BadRequestException(
        'AI ì‘ë‹µ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.',
      );
    }
  }

  /**
   * ì»¨í…ìŠ¤íŠ¸ ë©”ì‹œì§€ ìµœì í™” - GPT-4 Turboì˜ ëŒ€ìš©ëŸ‰ ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° í™œìš©
   */
  private async optimizeContextMessages(
    messages: Message[],
    maxMessages: number,
  ): Promise<Message[]> {
    if (messages.length <= maxMessages) {
      return messages; // ë©”ì‹œì§€ê°€ ì ìœ¼ë©´ ëª¨ë‘ í¬í•¨
    }

    // ìŠ¤ë§ˆíŠ¸ ì»¨í…ìŠ¤íŠ¸ ì„ íƒ ì „ëµ:
    // 1. ìµœê·¼ ë©”ì‹œì§€ë“¤ì„ ìš°ì„ ìˆœìœ„ë¡œ ì„ íƒ
    // 2. í† í° ìˆ˜ë¥¼ ê³ ë ¤í•˜ì—¬ ì¡°ì •
    // 3. ëŒ€í™”ì˜ ì—°ì†ì„±ì„ ìœ ì§€

    const recentMessages = messages.slice(-maxMessages);

    // í† í° ìˆ˜ ê³„ì‚°í•˜ì—¬ 128K ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° ë‚´ì—ì„œ ìµœì í™” (GPT-4o ê¸°ì¤€)
    let totalTokens = 0;
    const maxContextTokens = 120000; // 128Kì—ì„œ ì—¬ìœ ë¶„ í™•ë³´ (GPT-4o/GPT-5 ê³µí†µ)
    const optimizedMessages: Message[] = [];

    // ìµœê·¼ ë©”ì‹œì§€ë¶€í„° ì—­ìˆœìœ¼ë¡œ ì¶”ê°€
    for (let i = recentMessages.length - 1; i >= 0; i--) {
      const message = recentMessages[i];
      const messageTokens = this.estimateTokenCount(message.content);

      if (totalTokens + messageTokens <= maxContextTokens) {
        optimizedMessages.unshift(message); // ì•ì— ì¶”ê°€í•˜ì—¬ ìˆœì„œ ìœ ì§€
        totalTokens += messageTokens;
      } else {
        // í† í° í•œë„ì— ë„ë‹¬í•˜ë©´ ì¤‘ë‹¨
        this.logger.debug(
          `ì»¨í…ìŠ¤íŠ¸ í† í° í•œë„ ë„ë‹¬: ${totalTokens}/${maxContextTokens} í† í°, ${optimizedMessages.length}ê°œ ë©”ì‹œì§€ ì‚¬ìš©`,
        );
        break;
      }
    }

    this.logger.debug(
      `ì»¨í…ìŠ¤íŠ¸ ìµœì í™” ì™„ë£Œ: ì „ì²´ ${messages.length}ê°œ ì¤‘ ${optimizedMessages.length}ê°œ ë©”ì‹œì§€ ì„ íƒ (${totalTokens} í† í°)`,
    );

    return optimizedMessages;
  }

  /**
   * OpenAI APIë¥¼ ì‚¬ìš©í•œ ëŒ€í™” ìš”ì•½ ìƒì„±
   */
  async generateConversationSummary(conversationId: number): Promise<string> {
    try {
      const messages =
        await this.messageService.findConversationMessages(conversationId);

      if (messages.length === 0) {
        return 'ëŒ€í™”ê°€ ì—†ìŠµë‹ˆë‹¤.';
      }

      // GPT-4 Turboì˜ ëŒ€ìš©ëŸ‰ ì»¨í…ìŠ¤íŠ¸ë¥¼ í™œìš©í•˜ì—¬ ë” ë§ì€ ë©”ì‹œì§€ í¬í•¨
      const maxSummaryMessages = 100; // ìš”ì•½ì— ìµœëŒ€ 100ê°œ ë©”ì‹œì§€ í¬í•¨
      const summaryMessages = messages.slice(-maxSummaryMessages);

      // ì»¨í…ìŠ¤íŠ¸ê°€ ë§¤ìš° ê¸´ ê²½ìš° í† í° ìˆ˜ ì²´í¬
      const conversationText = summaryMessages
        .map((msg) => `${msg.role}: ${msg.content}`)
        .join('\n');

      const totalTokens = this.estimateTokenCount(conversationText);
      this.logger.debug(
        `ëŒ€í™” ìš”ì•½ ìƒì„±: ${summaryMessages.length}ê°œ ë©”ì‹œì§€, ì•½ ${totalTokens} í† í°`,
      );

      const response = await this.openai.chat.completions.create({
        model: this.selectedModel, // ì„ íƒëœ ìµœì‹  ëª¨ë¸ë¡œ ê³ í’ˆì§ˆ ìš”ì•½
        messages: [
          {
            role: 'system',
            content: `ë‹¤ìŒ ëŒ€í™”ë¥¼ í•œêµ­ì–´ë¡œ ìƒì„¸í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”. 
ì£¼ìš” ì£¼ì œ, í•µì‹¬ ë‚´ìš©, ëŒ€í™”ì˜ íë¦„, ì¤‘ìš”í•œ ê²°ë¡ ì„ í¬í•¨í•˜ì—¬ 
5-8ë¬¸ì¥ ì •ë„ë¡œ í¬ê´„ì ì¸ ìš”ì•½ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.
ëŒ€í™” ì°¸ì—¬ìë“¤ì˜ ê°ì •ì´ë‚˜ ì˜ë„ë„ íŒŒì•…í•  ìˆ˜ ìˆë‹¤ë©´ í¬í•¨í•´ì£¼ì„¸ìš”.`,
          },
          {
            role: 'user',
            content: conversationText,
          },
        ],
        max_tokens: 800, // GPT-4 Turboë¡œ ë” ìƒì„¸í•œ ìš”ì•½ ìƒì„±
        temperature: 0.3, // ìš”ì•½ì€ ì¼ê´€ì„± ìˆê²Œ
      });

      return (
        response.choices[0]?.message?.content || 'ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'
      );
    } catch (error) {
      this.logger.error('ëŒ€í™” ìš”ì•½ ìƒì„± ì‹¤íŒ¨', error);
      return 'ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.';
    }
  }

  /**
   * í† í° ìˆ˜ ì¶”ì • (OpenAI í† í° ê³„ì‚° ê·¼ì‚¬ì¹˜)
   */
  private estimateTokenCount(text: string): number {
    // OpenAIì˜ í† í° ê³„ì‚° ê·¼ì‚¬ì¹˜
    // - ì˜ì–´: ëŒ€ëµ 4ê¸€ì = 1í† í°
    // - í•œêµ­ì–´: ëŒ€ëµ 2-3ê¸€ì = 1í† í° (í•œê¸€ì€ ë” íš¨ìœ¨ì )

    // í•œê¸€ê³¼ ì˜ì–´ë¥¼ êµ¬ë¶„í•˜ì—¬ ê³„ì‚°
    const koreanChars = (text.match(/[ê°€-í£]/g) || []).length;
    const englishChars = (text.match(/[a-zA-Z]/g) || []).length;
    const otherChars = text.length - koreanChars - englishChars;

    // í•œê¸€: 2.5ê¸€ìë‹¹ 1í† í°, ì˜ì–´: 4ê¸€ìë‹¹ 1í† í°, ê¸°íƒ€: 3ê¸€ìë‹¹ 1í† í°
    const koreanTokens = Math.ceil(koreanChars / 2.5);
    const englishTokens = Math.ceil(englishChars / 4);
    const otherTokens = Math.ceil(otherChars / 3);

    return koreanTokens + englishTokens + otherTokens;
  }

  /**
   * ëŒ€í™” ìš”ì•½ ìƒì„± (í…ŒìŠ¤íŠ¸ìš© - ì†Œìœ ê¶Œ í™•ì¸ ì—†ìŒ)
   */
  async getConversationSummary(
    conversationId: number,
    userId?: number, // í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì„ íƒì 
  ): Promise<{ summary: string }> {
    try {
      const summary = await this.generateConversationSummary(conversationId);
      return { summary };
    } catch (error) {
      this.logger.error('Failed to generate conversation summary', error);
      return { summary: 'ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.' };
    }
  }

  /**
   * ì‚¬ìš©ì ë©”ì‹œì§€ í†µê³„ (í…ŒìŠ¤íŠ¸ìš© - ê°„ë‹¨í•œ êµ¬í˜„)
   */
  async getUserMessageStats(userId: number) {
    try {
      // í…ŒìŠ¤íŠ¸ìš© ê°„ë‹¨í•œ í†µê³„ (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ì¿¼ë¦¬ í•„ìš”)
      const totalMessages = await this.messageService.messageRepository.count({
        where: { role: MessageRole.USER },
      });

      const totalConversations =
        await this.conversationService.conversationRepository.count({
          where: { isActive: true },
        });

      return {
        totalMessages,
        totalConversations,
        averageMessagesPerConversation:
          totalConversations > 0
            ? Math.round(totalMessages / totalConversations)
            : 0,
        note: 'í…ŒìŠ¤íŠ¸ìš© í†µê³„ - ëª¨ë“  ì‚¬ìš©ì ë°ì´í„° í¬í•¨',
      };
    } catch (error) {
      this.logger.error('Failed to get user message stats', error);
      return {
        totalMessages: 0,
        totalConversations: 0,
        averageMessagesPerConversation: 0,
        error: 'í†µê³„ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.',
      };
    }
  }
}
